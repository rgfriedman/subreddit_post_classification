{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d153f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c68931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in cleaned data\n",
    "\n",
    "df = pd.read_csv('../Data/cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23fc65a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>women of reddit when you are in a relationship...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do you have a hot take what is it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why do you cry so much over little things</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>as an adult how do you make more female friends</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>which 3rd party reddit app do you use and what...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  subreddit\n",
       "0  women of reddit when you are in a relationship...          1\n",
       "1                  do you have a hot take what is it          1\n",
       "2          why do you cry so much over little things          1\n",
       "3    as an adult how do you make more female friends          1\n",
       "4  which 3rd party reddit app do you use and what...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87aa926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3e93d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5\n",
       "1    0.5\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline score\n",
    "\n",
    "df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4917c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create stop words to use in modeling\n",
    "\n",
    "stop_words =['men', 'women', 'reddit', 'guy', 'guys', 'woman', 'man', 'girl', 'girls', 'ladies', 'like', 'just', 've' ]\n",
    "\n",
    "stop_english = stopwords.words('english')\n",
    "\n",
    "stop = stop_words + stop_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdab13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try count vectorizer on tokenized titles. use gridsearch to find best params for count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10d5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variables for modeling\n",
    "\n",
    "X = df['title']\n",
    "\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ded61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split for model evaluation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2685c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build pipeline for gridsearch\n",
    "#got idea from 5.03 nlp notes\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', BernoulliNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a26f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features':[1_000, 3_000, 5_000, 7_000],\n",
    "    'cvec__stop_words'  :[None, 'english', stop],\n",
    "    'cvec__ngram_range' :[(1,1), (1,2), (2,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4701b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, param_grid = pipe_params, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82fa8363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', BernoulliNB())]),\n",
       "             param_grid={'cvec__max_features': [1000, 3000, 5000, 7000],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
       "                         'cvec__stop_words': [None, 'english',\n",
       "                                              ['men', 'women', 'reddit', 'guy',\n",
       "                                               'guys', 'woman', 'man', 'girl',\n",
       "                                               'girls', 'ladies', 'like',\n",
       "                                               'just', 've', 'i', 'me', 'my',\n",
       "                                               'myself', 'we', 'our', 'ours',\n",
       "                                               'ourselves', 'you', \"you're\",\n",
       "                                               \"you've\", \"you'll\", \"you'd\",\n",
       "                                               'your', 'yours', 'yourself',\n",
       "                                               'yourselves', ...]]})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a4661c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score:  0.7428888888888889\n",
      "testing score: 0.7175111111111111\n"
     ]
    }
   ],
   "source": [
    "print(f'training score:  {gs.score(X_train, y_train)}')\n",
    "\n",
    "print(f'testing score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8a519c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': 7000,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__stop_words': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is pretty decent. \n",
    "# The training and testing scores are similar, showing that the model performs similarly on seen and unseen data. \n",
    "# I was surprised that the best parameter for stop words was none. The ngram range makes sense for how people speak online.\n",
    "# Next I will explore other NLP tecniques or tweaking features to see if I can improve on this anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f1d2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column in df to try lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a32b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer. \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b34ecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['title_lem'] =  [list(i.split()) for i in df['title'].tolist() ]\n",
    "\n",
    "df['title_lem'] =[ ' '.join([lemmatizer.lemmatize(word) for word in row ]) for row in df['title_lem'] ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column in df to try stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e397a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db8057b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_stem'] =  [list(i.split()) for i in df['title'].tolist() ]\n",
    "\n",
    "df['title_stem'] =[ ' '.join([p_stemmer.stem(word) for word in row ]) for row in df['title_stem'] ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a93f2be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_lem</th>\n",
       "      <th>title_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>women of reddit when you are in a relationship...</td>\n",
       "      <td>1</td>\n",
       "      <td>woman of reddit when you are in a relationship...</td>\n",
       "      <td>women of reddit when you are in a relationship...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do you have a hot take what is it</td>\n",
       "      <td>1</td>\n",
       "      <td>do you have a hot take what is it</td>\n",
       "      <td>do you have a hot take what is it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why do you cry so much over little things</td>\n",
       "      <td>1</td>\n",
       "      <td>why do you cry so much over little thing</td>\n",
       "      <td>whi do you cri so much over littl thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>as an adult how do you make more female friends</td>\n",
       "      <td>1</td>\n",
       "      <td>a an adult how do you make more female friend</td>\n",
       "      <td>as an adult how do you make more femal friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>which 3rd party reddit app do you use and what...</td>\n",
       "      <td>1</td>\n",
       "      <td>which 3rd party reddit app do you use and what...</td>\n",
       "      <td>which 3rd parti reddit app do you use and what...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  subreddit  \\\n",
       "0  women of reddit when you are in a relationship...          1   \n",
       "1                  do you have a hot take what is it          1   \n",
       "2          why do you cry so much over little things          1   \n",
       "3    as an adult how do you make more female friends          1   \n",
       "4  which 3rd party reddit app do you use and what...          1   \n",
       "\n",
       "                                           title_lem  \\\n",
       "0  woman of reddit when you are in a relationship...   \n",
       "1                  do you have a hot take what is it   \n",
       "2           why do you cry so much over little thing   \n",
       "3      a an adult how do you make more female friend   \n",
       "4  which 3rd party reddit app do you use and what...   \n",
       "\n",
       "                                          title_stem  \n",
       "0  women of reddit when you are in a relationship...  \n",
       "1                  do you have a hot take what is it  \n",
       "2            whi do you cri so much over littl thing  \n",
       "3      as an adult how do you make more femal friend  \n",
       "4  which 3rd parti reddit app do you use and what...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try cvec with lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "228175f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variables for modeling with lemmatizer\n",
    "\n",
    "X = df['title_lem']\n",
    "\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f3ba5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', BernoulliNB())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features':[3_000, 5_000, 7_000, 10_000],\n",
    "    'cvec__stop_words'  :[None, 'english', stop],\n",
    "    'cvec__ngram_range' :[(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = pipe_params, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fa94414b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', BernoulliNB())]),\n",
       "             param_grid={'cvec__max_features': [3000, 5000, 7000, 10000],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
       "                         'cvec__stop_words': [None, 'english',\n",
       "                                              ['men', 'women', 'reddit', 'guy',\n",
       "                                               'guys', 'woman', 'man', 'girl',\n",
       "                                               'girls', 'ladies', 'like',\n",
       "                                               'just', 've', 'i', 'me', 'my',\n",
       "                                               'myself', 'we', 'our', 'ours',\n",
       "                                               'ourselves', 'you', \"you're\",\n",
       "                                               \"you've\", \"you'll\", \"you'd\",\n",
       "                                               'your', 'yours', 'yourself',\n",
       "                                               'yourselves', ...]]})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "07b57add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score:  0.7474814814814815\n",
      "testing score: 0.7199111111111111\n"
     ]
    }
   ],
   "source": [
    "print(f'training score:  {gs.score(X_train, y_train)}')\n",
    "\n",
    "print(f'testing score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "51128a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': 10000,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__stop_words': None}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model scores almost exactly the same as the one before, showing that lemmatizing does not make a big difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try cvec with stemming. use grisearch to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42101c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variables for modeling with stemming\n",
    "\n",
    "X = df['title_stem']\n",
    "\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "74e334a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', BernoulliNB())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features':[5_000, 7_000, 10_000],\n",
    "    'cvec__stop_words'  :[None, 'english', stop],\n",
    "    'cvec__ngram_range' :[(1,1), (1,2), (2,2)]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = pipe_params, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a2f9f707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', BernoulliNB())]),\n",
       "             param_grid={'cvec__max_features': [5000, 7000, 10000],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
       "                         'cvec__stop_words': [None, 'english',\n",
       "                                              ['men', 'women', 'reddit', 'guy',\n",
       "                                               'guys', 'woman', 'man', 'girl',\n",
       "                                               'girls', 'ladies', 'like',\n",
       "                                               'just', 've', 'i', 'me', 'my',\n",
       "                                               'myself', 'we', 'our', 'ours',\n",
       "                                               'ourselves', 'you', \"you're\",\n",
       "                                               \"you've\", \"you'll\", \"you'd\",\n",
       "                                               'your', 'yours', 'yourself',\n",
       "                                               'yourselves', ...]]})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6b4d89b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score:  0.7506814814814815\n",
      "testing score: 0.7220888888888889\n"
     ]
    }
   ],
   "source": [
    "print(f'training score:  {gs.score(X_train, y_train)}')\n",
    "\n",
    "print(f'testing score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8185bf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': 10000,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__stop_words': None}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8217d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model scores similarly to the two above, just ever so slightly higher. \n",
    "# Stemming does not seem to make a significant difference for the data in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bdf174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All count vectorizer models with tokenized, lemmatized, and stemmed titles perform pretty similarly. \n",
    "# They are all scoring similarly and slightly overfit (around .74 for training scores and around .71 for testing scores)\n",
    "# The scores are better than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80894b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try tfidf with stemmimg. use grisearch to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4b7db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variables for modeling with stemming\n",
    "\n",
    "X = df['title_stem']\n",
    "\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa4a307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer() ),\n",
    "    ('nb', MultinomialNB() )\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features':[5_000, 7_000, 10_000],\n",
    "    'tvec__stop_words'  :[None, 'english', stop],\n",
    "    'tvec__ngram_range' :[(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = pipe_params, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7f24e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'tvec__max_features': [5000, 7000, 10000],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tvec__stop_words': [None, 'english',\n",
       "                                              ['men', 'women', 'reddit', 'guy',\n",
       "                                               'guys', 'woman', 'man', 'girl',\n",
       "                                               'girls', 'ladies', 'like',\n",
       "                                               'just', 've', 'i', 'me', 'my',\n",
       "                                               'myself', 'we', 'our', 'ours',\n",
       "                                               'ourselves', 'you', \"you're\",\n",
       "                                               \"you've\", \"you'll\", \"you'd\",\n",
       "                                               'your', 'yours', 'yourself',\n",
       "                                               'yourselves', ...]]})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d035f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score:  0.7577481481481482\n",
      "testing score: 0.7177777777777777\n"
     ]
    }
   ],
   "source": [
    "print(f'training score:  {gs.score(X_train, y_train)}')\n",
    "\n",
    "print(f'testing score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a19ed3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__max_features': 10000,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__stop_words': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF with multinomial nb scores similarly to models above too. \n",
    "# This is still scoring better than the baseline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
